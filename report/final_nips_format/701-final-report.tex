\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09
\usepackage{pifont}
\usepackage{times,amsmath,color, balance,tabularx,caption,
amssymb,graphicx,epsfig,cite,psfrag,subfigure,algorithm,multirow,cases,algorithmic,mathtools}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{adjustbox}

\newtheorem{claim}{Claim}
\newtheorem{guess}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{fact}{Fact}
\newtheorem{assumption}{\underline{Assumption}}
\newtheorem{theorem}{\underline{Theorem}}
\newtheorem{lemma}{\underline{Lemma}}
\newtheorem{ctheorem}{Corrected Theorem}
\newtheorem{corollary}{\underline{Corollary}}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{\underline{Example}}
\newtheorem{remark}{\underline{Remark}}
\newtheorem{problem}{\underline{Problem}}
\def\Ei{\mathop\mathrm{Ei}}
\def\E{\mathop\mathrm{E}}
\def\tr{\mathop\mathrm{tr}}
\newcounter{mytempeqncnt}

\title{Decoding Team Composition in MOBA Games \\ A Learning Approach}


\author{
Jiachen Li$^*$, Yilin He$^\dag$, and Chengliang Lian$^*$\\
Language Technologies Institute$^*$ and Machine Learning Department$^\dag$ \\ School of Computer Science \\
Carnegie Mellon University, Pittsburgh, PA, 15213 \\
\texttt{\{jiachenl, yilinhe, clian\}@andrew.cmu.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
Team composition has been a interesting as well as challenging problem especially in the setting of competitive online games. A good team composition will strongly improve your chance for wining the game, while a bad team composition may probably ruin your game.
In this project, we studied the team composition in the massive online battle arena (MOBA) games, and tried to analyze the impact of team composition on the game result. The data we used are collected from two most popular MOBA games DotA 2 and LoL, and we proposed two feature models to represent these data. Machine learning algorithms with different characteristics such as logistic regression (LR), support vector machine (SVM) and deep belief networks (DBN) are used to train the predicting model, and our final results shows that the team composition does have influence on game results and this relationship is decodable.
\end{abstract}

\section{Introduction}


%\begin{center}
%   \url{http://papers.nips.cc}
%\end{center}

Multiplayer online battle arena (MOBA) is a rising force in online games that has features of fast paced, single round and team play. In the past few months, more than 27 million players fight with each other per day in one of such kind of game called League of Legends (LoL)\cite{Ian} while millions are attracted by another game called Defense of the Ancient 2 (DotA2). International MOBA tournaments are held over the world with a prize pool of over \$10 million\cite{Valve}.  Analysis on MOBA games will not only gain us experience of data analysis but will also be beneficial to players and game companies all over the world.

In MOBA games like LoL or DotA 2, two groups of five players are pitted against each other based on their level of game proficiency. In the beginning of a game, each player first picks an unique character out of over 100 options, and these 5 characters picked in a group finally forms a team. Given that the game server usually matches the players with similar levels within a game, then the result of a game will not be influenced too much by the skill of each individual player, but depends on the combination of
picked characters in each group, which is known as team composition.

An interesting question to the online communities as well as professional teams is how to form a good team composition that can achieve the highest probability to win the game. The challenging part for this problem is the large number of possible team compositions. To be specific, since there are generally more than 100 characters for players
to choose, the number of possible team compositions for both teams are more than $\binom{100}{10}\cdot\binom{10}{5}\approx ?????$.


To answer this question, we can transform it into a classification problem in machine learning: given a team composition, predict the result for the game. With this problem transformation, we can learn how to form a good team composition by interpreting the learning model parameters and then form good team composition in practice by using the learned model for prediction and recommendation.

In the midpoint, we have successfully fulfilled the goal defined in the proposal.
We collected and cleaned up data for both LoL and DotA 2.
We built a model to predict game outcome and evaluated it using logistic regression and support vector machine.
Using the simple model, we achieved an accuracy of $60.62\%$ for LoL and $63.48\%$ for DotA 2.


%\subsection{Related Work}
%Conley and Perry have previously worked on predicting win ratio based on heroes picked by the team in DotA 2. In their work, they have reported 69\% accuracy using logistic regression and K-NN.

%\subsection{Organization of this Report}
%The remaining of this report is organized as follows: Section II presents the collection of the data set, Section III introduces the learning model and assumptions,
\section{Data Set Preparation}
We have obtained two data sets for this study, one from the LoL and the other from DotA 2. The whole data set contains more than 80,000 independent game history records in total, which are adequate for this project.

\subsection{Data Collection}

The data set was collected through public APIs provided by the video game producers Riot and Steam respectively. The APIs can provide us with a given number of most recent completed games as well as the corresponding whole statistics. To make full use of the APIs, we ran our feature collection program on AWS Cloud Computing resources to collect these data in a $7\times24$ manner.


\subsection{Data Contents}

The contents of the original data collected from APIs are shown as follow:


\begin{itemize}
\item \textit{Game Id}: An Unique identifier for the game, used to avoid data redundancy;
\item \textit{Game duration}: how long the each game last, used for data cleaning;
\item \textit{Winner}: The winning team, used as labels;
\item \textit{Team composition}: the characters selected by each team, used as main features;
\item \textit{Skill levels}: the player's performance in his/her past history, used to evaluate the potential performance of this player;
\item \textit{KDA Statistics}: the character's performance in this game, i.e., how many other characters it killed (K), how many times it died (D) and how many times it assisted its teammate to kill the other characters (A).
\end{itemize}

\subsection{Data Cleaning}

As we all known that the performance of machine learning algorithms is heavily dependent on the quality of features. So before extracting features from raw data, we need to do some data cleaning.

Since the public APIs always return the most recent game records, we should first to check whether there is any redundancy in our data set. Besides, we need to make sure the data is useful, which can be verified from the game duration. For example, if the game duration is too short, then it is quite possible that some player leaves the game early, so that this record may not be able to support our learning goal and should be dropped. Finally, we will have a look at the KDA statistics, if some character's KDA is below a normal level, then this character may not fully participate into the game. Therefore, the corresponding record should also be dropped.

After finishing these procedures, we can then extract features from the data and train our models.






\section{Models and Assumptions}

In this section, we proposed two models for decoding team composition. The first model is based on ranking the performance of each character while the second model takes the group effect into consideration. We take the first model as the baseline, and mainly apply our ideas in the second model.


To start discussing the models,  let us first define the two teams in the game as $\mathcal{A}$ and $\mathcal{B}$, where each team contains five unique characters (i.e., $c_1,c_2\ldots,c_5$). Moreover, suppose that the game has $N$ characters in total, and all the characters can be encoded with an integer between $1$ and $N$.

\subsection{Model-I: Character Ranking Model}

In the character ranking model, what we really want to learn is the ``weight'' of different characters, which measures how important the characters to a team. To introduce the that, we may first need to make some assumption:

\begin{assumption}
We assume that players in both team all have high skills. Moreover, they are proficient with the characters they pick.
\end{assumption}

\begin{remark}
This assumption is reasonable because, during the data collection part, we only collected the records with a higher skill levels. Moreover, in order to win the game, players are not likely to pick the characters that they are not familiar with, especially for the high level player.
\end{remark}

With this assumption, we can further assume that the game result is independent of the players, then we can use the vector $\textbf{x}=\{x_1,x_2,\ldots,x_N\}$ to represent the training feature. The element $x_i$ indicates the presence of $i$th character in the game, and the detailed value for it is given by

\begin{equation}
x_i =
\begin{cases}
1 &  \text{if $i\in \mathcal{A},$} \\
-1 &  \text{if $i\in \mathcal{B},$} \\
0 & \text{otherwise.}
\end{cases}
\end{equation}
Of course, the winner team will work as the label of the feature.


\subsection{Model-II: Group Ranking Model}

\begin{figure}[t]
  \centering
    \includegraphics[width=65mm]{team_comp.pdf}
  \caption{An illustration of team composition and the aspects that may influences the game result.}
  \label{fig:team_comp}
\end{figure}

Unlike the character ranking model where we make the independent assumption between player skills and game results. The group ranking model not only considers the player skills, but also take the relationship of different characters into account. For example, Fig. \ref{fig:team_comp} has shown the factors that may have potential influence on the game result. It can be seen from the figure that the performance of one specific character should affect all the other characters in the different team. This is not difficult to understand because there are many times in the game that all the characters in a team will need to work together to achieve their goals. Therefore, to learn the team composition, we need also use such information.

To utilize these additional information, we first introduce the character matrix.
\begin{definition}
A matrix $A \in \mathbb{R}^{N\times N}$ is called the character matrix if the element $a_{i,j}$ indicates the probability that the team with character $i$ beats the team with character $j$, i.e. $a_{i,j}=\text{Pr}\{\text{team with $c_i$ beats team with $c_j$}\}$.
\end{definition}

The character matrix gives us an evaluation between different character pair that can be used for learning. However, to obtain a more complete feature we may also need to consider the impact of the players (i.e., use a function $g(P_{\mathcal{A},i},P_{\mathcal{B},j})$ to evaluate the winning ratio between two players), spo the final feature could be a matrix $F \in \mathbb{R}^{N\times N}$ based on this information. The element of feature matrix is defined as
\begin{equation}
f_{i,j}=a_{i,j} \cdot g(P_{\mathcal{A},i},P_{\mathcal{B},j}),
\end{equation}
where the $a_{i,j}$ is the element from character matrix, $g(P_{\mathcal{A},i},P_{\mathcal{B},j})$ is the function that measure the winning ratio between the player in group $\mathcal{A}$ using character $i$ and the player in group $\mathcal{B}$ using character $j$. Notice that we can multiply these two elements because they are independent.

\begin{remark}
There is no doubt that the feature matrix contains more information than the plain character feature in the character ranking model. However, one of the biggest challenge when applying the feature matrix is its high dimension (i.e. $N\times N$, which will also lead to the sparsity of the parameters). To solve this problem, we will use the clustering methods to cluster the $N$ characters into $K$ groups, so that the comparison between two individual characters will become the comparison between two clusters. Thus the dimension problem will be reduced. (This will be the one of the work during the second part of this semester.)
\end{remark}



\section{Performance Evaluation}



\begin{figure}[t]
  \centering
    \includegraphics[width=90mm]{dota2_logreg_error.pdf}
  \caption{The error probability of logistic regression on the DotA 2 dataset.}
  \label{fig:dota2_log}
\end{figure}


\begin{figure}[t]
  \centering
    \includegraphics[width=90mm]{lol_logreg_error.pdf}
  \caption{The error probability of logistic regression on the LoL dataset.}
  \label{fig:lol_log}
\end{figure}



\subsection{Evaluation of logistic regression}

Fig. 2 and 3 show the error probability versus training sample on two dataset with Logistic Regression, We select a learning rate of $\alpha = 0.001$, the proportion of test set is 30\%, which is randomly picked from the dataset. For each data set, the iteration time for gradient decent is 100 during the training.

This figure represents the learning curve for logistic regression algorithm. With small number of training examples, we obtained a small training error and a large testing error. As we increase the number of training examples, the training error increase and testing error decreases. For DotA 2, both test error and training error converges to $36.5\%$ after around 10,000 training points.

The weight vector we obtained in logistic regression represents each character's influence on game result. For character $i$, a positive weight $w_i$ implies that picking this character will increase the chance of winning. Therefore, the prediction is based on individual characters and does not consider synergy between characters.

\subsection{Evaluation of neural network}
In neural networks model, we choose to use 3 layers: 1 hidden layer with 100 nodes. The input layer has p nodes and the output layer has 2 nodes. We implemented feedforward back propagation neural network. Sigmoid transform was applied and lambda in cost function was set to 0.5. (Since the evaluation of error probability versus the number of sample is too expensive for neural networks, we didn't do that in this midway report, but we will add it in the final report)

\subsection{Comparison of algorithms}

\begin{table}[ht]
\center
\captionsetup{font={small}}
\caption{performance of different algorithms}
\begin{tabular}{|c |c |c |c |c |}
\hline
& \multicolumn{2}{c|}{LoL}    & \multicolumn{2}{c|}{Dota 2}   \\
\hline
Accuracy (\%)                       & Training   & Test  & Training  & Test  \\ \hline
Logistic Regression    & 62.55         & 60.62     & 63.06         & 63.48     \\ \hline
Neural Networks       & 72.63        & 70.77     & 65.12        &64.81     \\ \hline
\end{tabular}
\end{table}

While we can not directly interpret the parameters we obtained from neural networks model, the high accuracy shows the importance of considering the relationship between selected characters. Since logistic regression has a linear decision boundary, it can only make prediction based on a linear combination of individual characters. Therefore, logistic regression can not capture the relationship between selected characters. On the other side, the learning process of Neural Network combines input feature in the hidden layer to take consideration of the relationship between selected characters.

\subsection{Comparison of data sets}
Logistic regression yield similar accuracy for LoL and DotA 2 while neural networks has better result on LoL than DotA 2. This result from logistic regression implies that in LoL and DotA 2, selection of individual characters have similar impact on game result. One possible explanation for higher accuracy in LoL data set when using neural networks is that the team composition in LoL focus more on the synergy between characters, thus considering more complicated relationship between characters gives better result.



\section{Conclusion and Future Work}

Team composition has been a interesting as well as challenging problem especially in the setting of competitive online games. A good team composition will strongly improve your chance for wining, while a bad team composition may probably ruin your game.

In this report, we studied the team composition in the massive online battle arena (MOBA) games, and tried to decode the impact of team composition.
During first half of the semester, we finished the goal we set in the proposal including:
\begin{itemize}
\item collect data set from LoL and DotA 2
\item build feature model
\item run logistic regression on each data set
\item run neural network on each data set
\end{itemize}


Our current results shows that the team composition does have connection with game results and the relationship is encodable. However, due to the limitation of original model, we proposed a second model to encode the relationship between selected characters and the players.

For the remaining semester, our plan is shown as follows

Nov 1 - Nov 15: Set up and run logistic regression and deep learning algorithm on our dataset using new model.

Nov 16 - Nov 30: Analysis our result and tune parameters for different algorithm.

To make the project successful, the minimum requirement is not longer the data collection, pre-processing and algorithms implementation, but to validate our model and make a better analysis and learn well.





\subsubsection*{Acknowledgments}

We would like to thank Nicole and Prof. Gordon for their....

\subsubsection*{References}


References follow the acknowledgments. Use unnumbered third level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to `small' (9-point)
when listing the references. {\bf Remember that this year you can use
a ninth page as long as it contains \emph{only} cited references.}

\small{

[1] Ian Sherr, ``Player Tally for League of Legends Surges,'' \textit{Digits RSS. N.p., n.d. Web}. 28 Sept, 2014.

[2] Valve, Dota 2 -- 2014 Compendium -- The International. Dota 2 Official Website.

Note that the following part is just for reference!

[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
and T.K. Leen (eds.), {\it Advances in Neural Information Processing
Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
Realistic Neural Models with the GEneral NEural SImulation System.}
New York: TELOS/Springer-Verlag.

[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
and recall at excitatory recurrent synapses and cholinergic modulation
in rat hippocampal region CA3. {\it Journal of Neuroscience}
{\bf 15}(7):5249-5262.
}

\end{document}
